AWS:
------------------------------------------------------------------------------------------------------
16/04/2023
-------------
--aws free tier account
https://aws.amazon.com/free/?trk=09863622-0e2a-4080-9bba-12d378e294ba&sc_channel=ps&ef_id=Cj0KCQjwi46iBhDyARIsAE3nVrZOgIvijk2GXa93CUSbSOjCfZdvx_X5Pr0IB8aQAgXxvr2jYwM9ot0aAlB1EALw_wcB:G:s&s_kwcid=AL!4422!3!453325184854!e!!g!!aws%20free%20tier!10712784862!111477280251&all-free-tier.sort-by=item.additionalFields.SortRank&all-free-tier.sort-order=asc&awsf.Free%20Tier%20Types=*all&awsf.Free%20Tier%20Categories=*all

tier type : 12 months free/always free

---------------------
--S3
Create bucket -> bigdata-s3-bucket-batch2 , mumbai region , tag:(key:environment - Value:dev)
1.upload -> add files -> browse and select file
2.Create folder -> folder2 -> file.csv

--lamda
Create function (mumbai region) -> batch2-lamda-demo, python 3.9
1.General Configuration
basic setting -> timeout: 15 min -> save

2.code source : lambda_function.py
configure test event -> test-function, event json ( "m" : 10 , "n" : 5)


info of any resource/object:
s3 URL: s3:///bucket-name/directory-path/datafile.csv (here key = directory-path/datafile.csv)
ARN(Amazon Resource Name) - address of a resource which can be used for establishing connection or for blocking purposes

-------------
22/04/2023
-------------
S3 Bucket,Lambda function,IAM

# Creating a lambda function which triggers as soon as a file lands/uploaded to a s3 bucket (event driven pipeline).

1.Create function (mumbai region) -> lamdaDemo, python 3.9 (15 min timeout) -> save
Open lambda function and write the code (https://github.com/SHUBH-17/AWS/blob/main/access_s3_in_lambda.py) ->deploy
lambda->layers->add layer-> choose pandas and it's version ->add

2.s3 bucket -> bucket name -> properties -> event notification 
create event notification -> send-notification-to-s3, types: all object create events, lamda function: give arn or name of above function-> save

3.open IAM -> roles -> copy/select lambda role -> add permission (attach policy:s3FullAccess)

4.s3 bucket -> bucket name -> upload -> employees.csv

5.Open lambda function and check logs (Monitor) (use json formatter on web to visualize the json data from the log)

-------------
SNS (works on pull model):

1.Topic -> standard -> sns-topic -> create topic
Inside topic:
--create subscription -> protocol:email, endpoint:email address
--open mail and confirm subscription
--publish messgae

2.Alter lambda function and use code (https://github.com/SHUBH-17/AWS/blob/main/sns_notification_from_lambda.py)
note: sns_arn should be the arn of the topic created above

3.open IAM -> roles -> copy/select lambda role -> add permission (attach policy:SNSFullAccess)

4.s3 bucket -> bucket name -> upload -> employees.csv

5.open mail and check message is published
-------------
EC2:
1.launch instance 
name:ec2-demo
OS: amazon linux 2023 ami (free tier eligible)
instance type : t2.micro (free tier eligible)
key pair login -> create key pair -> ec2-key (.pem file gets downloaded)
launch instance

2.Come to ec2 homepage and click instances 
select instance -> connect -> SSH client -> copy command
open terminal  (ec2 command line) and go to directory where .pem file is located:
> chmod 400 *.pem (provide pem file name).
> paste the command to connect to the remote server/ec2 instance.
ec2 instance> mkdir shubh
ec2 instance> ls
ec2 instance>


select instance -> instance state -> stop/terminate (avoids unneccessary charges)

-------------
RDS 

1. RDS -> create database (easy create, MySQL, free tier, DB instance : demo-rds, username-password)

2.database -> demo-rds (status:active) -> Actions -> setup ec2 connection -> select ec2 instance (eg. ec2-demo created above if under same vpc as rds or ec2-for-rds-connect created on same vpc) -> confirm and setup
name:ec2-for-rds-connect 
ec2-key created above can be used
launch instance

3. creating tunnel 
Instance -> ec2-for-rds-connect -> connect -> connection details (hostname)
(details required: endpoint,port,username-password) (local port: 3336, local host: 127.0.0.1)
terminal on local machine where .pem file is located:
> ssh -i "ec2-key.pem" -L 3336:endpoint:port hostname -N -f
> lsof -i4 -P | grep -i "listen" | grep 3336
> nc -zv 127.0.0.1 3336
> mysql -h 127.0.0.1 -P 3336 -u admin -p
(MySQL instance):
mysql> show databases;  

pause/delete/terminate the cluster
-------------
Notes:
1.lambda has memory constraint and timeout constraint (15 min limit) and thats why cannot be used for heavy computation and loading high volume data.
(use lambda function to handle other libraries for triggering a job to do such heavy computaion and data load for any event driven pipeline)

documentation links:
https://boto3.amazonaws.com/v1/documentation/api/latest/guide/s3-examples.html

A virtual private cloud (VPC) is a private cloud computing environment contained within a public cloud.

Task: try connecting (MySQL instance) using python or workbench instead of using ec2 command line
------------------------------------------------------------------------------------------------------
-------------
23/04/2023
-------------
EMR (NO free tier service) - can be used for practising/using applications like hadoop, spark, hive etc  (service for distributed computation)

1.Create Cluster:
name: emr-demo
emr release : emr-5.36.0
Application bundle: custom bundle(hadoop,hue,spark,hcatalog,hive)
create key pair or use ec2-key created above
service role: emr-default-role
instance role: emr-ec2-default-role
if no roles are shown then create role:
IAM -> Roles -> Create Role -> aws service -> EMR -> EMR -> emr-default-role -> create role
IAM -> Roles -> Create Role -> aws service -> EMR -> EMR Role for EC2 -> emr-ec2-default-role -> create role
create cluster

once the cluster gets created and status is available:
-> click on link: connect to primary node using ssh
-> copy command
-> open local terminal where .pem file is located and paste the command
> command
> fingerprint? yes
hadoop> hdfs dfs -ls /
hadoop> hdfs dfs -mkdir /input_data (confirm on hdfs name node url present on the emr cluster)
hadoop> hive
hive> show databases;
hive> exit
hadoop> pyspark
>>> from pyspark.sql import SparkSession
>>> exit()
hadoop> clear
hadoop> spark-submit

pause/delete/terminate the cluster

-------------
Redshift (Data Warehousing service)
1.Create cluster:
name: demo-red-shift-cluster
select load sample data
admin user name: admin
admin password: admin
Associated IAM roles: create IAM role (any s3 bucket) -> Create
create cluster

once the cluster gets created and status is available:
-> click query data (editor v2)
under demo-red-shift-cluster and dev DB :
select * from public.category limit 100;

pause/delete/terminate the cluster

-------------
Glue (Managed ETL service)
#employee csv file uploaded on s3 bucket (source folder for the crawler)
--Crawlers and Catalog
1.Crawlers 
-> create crawler
Name: csv-crawler
Description: This will crawl the data which is residing in S3 bucket and file name is employees.csv
Add a data source -> S3 -> bucket name -> folder -> choose -> add 
select above data source
create IAM role (AWSGlueServiceRole-Glue-EZCRC-s3Policy ): s3FullAccess,dynamodbfullaccess,glueservicerole,redshifyfullaccess,glueconsolefullaccess
add database(to store metadata):
DB name: demo-db 
Table prefix: employee_
create crawler

select csv-crawler and click run (once completed check the table for metadata it created)
databases -> demo-db -> click on table name -> shows/stores the schema (metadata) of the source file

2.Glue Job
ETL jobs 

A.visual with a source and  target (simple drag and drop)
source: amazon s3
target: amazon redshift
create
click source -> data catalog table -> demo-db -> table name 
click destination -> redshift connection (create if not exist using the connection option in glue service)

B.Spark script editor
create (Employee_Data_Processing)
script: https://github.com/SHUBH-17/AWS/blob/main/glue_spark_script.py (confirm db and table name)
save
IAM role: AWSGlueServiceRole-Glue
Requested No of workers: 2
Job Parameter : key: --JOB_NAME, value: first_glue_job
save

ETL jobs -> Employee_Data_Processing -> run
click job run monitoring
cloudwatch logs

-------------
links and notes:
https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-master-core-task-nodes.html
https://docs.aws.amazon.com/redshift/index.html
https://docs.aws.amazon.com/redshift/latest/dg/c_redshift-sql.html
https://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-python-extensions.html

Crawlers can also validate the schema of the input source.
Catalog acts as a middle layer between the actual code and actual data source.

------------------------------------------------------------------------------------------------------
-------------
29/04/2023
-------------
Glue Job: Processing incremental data load
create a glue job Spark script editor ()
script: https://github.com/SHUBH-17/AWS/blob/main/incremental_data_load_in_glue.py  (confirm db and table name)
job property: enable bookmark

create Crawlers and Catalog

delete and upload the employees.csv file in s3
check count in logs :50

upload another file employees_2.csv file in s3 with 4 records
check count in logs :04



------------------------------------------------------------------------------------------------------
select instance -> instance state -> stop/terminate (avoids unneccessary charges)
------------------------------------------------------------------------------------------------------
