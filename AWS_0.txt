AWS:
------------------------------------------------------------------------------------------------------
16/04/2023
-------------
--aws free tier account
https://aws.amazon.com/free/?trk=09863622-0e2a-4080-9bba-12d378e294ba&sc_channel=ps&ef_id=Cj0KCQjwi46iBhDyARIsAE3nVrZOgIvijk2GXa93CUSbSOjCfZdvx_X5Pr0IB8aQAgXxvr2jYwM9ot0aAlB1EALw_wcB:G:s&s_kwcid=AL!4422!3!453325184854!e!!g!!aws%20free%20tier!10712784862!111477280251&all-free-tier.sort-by=item.additionalFields.SortRank&all-free-tier.sort-order=asc&awsf.Free%20Tier%20Types=*all&awsf.Free%20Tier%20Categories=*all

tier type : 12 months free/always free

---------------------
--S3
Create bucket -> bigdata-s3-bucket-batch2 , mumbai region , tag:(key:environment - Value:dev)
1.upload -> add files -> browse and select file
2.Create folder -> folder2 -> file.csv

--lamda
Create function (mumbai region) -> batch2-lamda-demo, python 3.9
1.General Configuration
basic setting -> timeout: 15 min -> save

2.code source : lambda_function.py
configure test event -> test-function, event json ( "m" : 10 , "n" : 5)


info of any resource/object:
s3 URL: s3:///bucket-name/directory-path/datafile.csv (here key = directory-path/datafile.csv)
ARN(Amazon Resource Name) - address of a resource which can be used for establishing connection or for blocking purposes

-------------
22/04/2023
-------------
S3 Bucket,Lambda function,IAM

# Creating a lambda function which triggers as soon as a file lands/uploaded to a s3 bucket (event driven pipeline).

1.Create function (mumbai region) -> lamdaDemo, python 3.9 (15 min timeout) -> save
Open lambda function and write the code (https://github.com/SHUBH-17/AWS/blob/main/access_s3_in_lambda.py) ->deploy
lambda->layers->add layer-> choose pandas and it's version ->add

2.s3 bucket -> bucket name -> properties -> event notification 
create event notification -> send-notification-to-s3, types: all object create events, lamda function: give arn or name of above function-> save

3.open IAM -> roles -> copy/select lambda role -> add permission (attach policy:s3FullAccess)

4.s3 bucket -> bucket name -> upload -> employees.csv

5.Open lambda function and check logs (Monitor) (use json formatter on web to visualize the json data from the log)

-------------
SNS (works on pull model):

1.Topic -> standard -> sns-topic -> create topic
Inside topic:
--create subscription -> protocol:email, endpoint:email address
--open mail and confirm subscription
--publish messgae

2.Alter lambda function and use code (https://github.com/SHUBH-17/AWS/blob/main/sns_notification_from_lambda.py)
note: sns_arn should be the arn of the topic created above

3.open IAM -> roles -> copy/select lambda role -> add permission (attach policy:SNSFullAccess)

4.s3 bucket -> bucket name -> upload -> employees.csv

5.open mail and check message is published
-------------
EC2:
1.launch instance 
name:ec2-demo
OS: amazon linux 2023 ami (free tier eligible)
instance type : t2.micro (free tier eligible)
key pair login -> create key pair -> ec2-key (.pem file gets downloaded)
launch instance

2.Come to ec2 homepage and click instances 
select instance -> connect -> SSH client -> copy command
open terminal  (ec2 command line) and go to directory where .pem file is located:
> chmod 400 *.pem (provide pem file name).
> paste the command to connect to the remote server/ec2 instance.
ec2 instance> mkdir shubh
ec2 instance> ls
ec2 instance>


select instance -> instance state -> stop/terminate (avoids unneccessary charges)

-------------
rds and emr

1. RDS -> create database (easy create, MySQL, free tier, DB instance : demo-rds, username-password)

2.database -> demo-rds (status:active) -> Actions -> setup ec2 connection -> select ec2 instance (eg. ec2-demo created above if under same vpc as rds or ec2-for-rds-connect created on same vpc) -> confirm and setup
name:ec2-for-rds-connect, 
ec2-key created above can be used
launch instance

A virtual private cloud (VPC) is a private cloud computing environment contained within a public cloud.


3. creating tunnel 
Instance -> ec2-for-rds-connect -> connect -> connection details (hostname)
(details required: endpoint,port,username-password) (local port: 3336, local host: 127.0.0.1)
terminal on local machine where .pem file is located:
> ssh -i "ec2-key.pem" -L 3336:endpoint:port hostname -N -f
> lsof -i4 -P | grep -i "listen" | grep 3336
> nc -zv 127.0.0.1 3336
> mysql -h 127.0.0.1 -P 3336 -u admin -p
(MySQL instance):
mysql> show databases;  


Task: try connecting (MySQL instance) using python and workbench instead of using ec2 command line

-------------
Notes:
1.lambda has memory constraint and timeout constraint (15 min limit) and thats why cannot be used for heavy computation and loading high volume data.
(use lambda function to handle other libraries for triggering a job to do such heavy computaion and data load for any event driven pipeline)


documentation links:
https://boto3.amazonaws.com/v1/documentation/api/latest/guide/s3-examples.html


------------------------------------------------------------------------------------------------------
-------------
23/04/2023
-------------




------------------------------------------------------------------------------------------------------




------------------------------------------------------------------------------------------------------
